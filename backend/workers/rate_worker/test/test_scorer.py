"""Tests for scorer helpers and metrics."""

from unittest.mock import Mock, patch

from src.performance_claims import calculate_performance_claims_with_timing
from src.ramp_up_time import calculate_ramp_up_time_with_timing
from src.scorer import (
    ScoreResult,
    calculate_code_bus_factor,
    calculate_dataset_bus_factor,
    calculate_model_bus_factor,
    calculate_size_score,
    estimate_model_size,
    is_major_organization,
    make_request,
    score_code,
    score_dataset,
    score_model,
    score_url,
)
from src.url import UrlCategory


class TestScoreResult:
    """Tests for ScoreResult helper."""

    def test_percentage_calculation(self):
        """Percent uses max_score when present."""
        result = ScoreResult(
            url="https://example.com",
            category=UrlCategory.MODEL,
            score=7.5,
            max_score=10.0,
            details={},
        )
        assert result.percentage == 75.0

    def test_percentage_zero_max_score(self):
        """Percent is zero when max_score is zero."""
        result = ScoreResult(
            url="https://example.com",
            category=UrlCategory.MODEL,
            score=5.0,
            max_score=0.0,
            details={},
        )
        assert result.percentage == 0.0

    def test_str_representation(self):
        """String representation includes category and scores."""
        result = ScoreResult(
            url="https://example.com",
            category=UrlCategory.MODEL,
            score=8.0,
            max_score=10.0,
            details={},
        )
        assert "MODEL" in str(result)
        assert "8.0/10.0" in str(result)
        assert "80.0%" in str(result)


class TestMakeRequest:
    """Tests for HTTP request wrapper."""

    @patch("src.scorer.requests.get")
    def test_successful_request(self, mock_get):
        """Return JSON when request succeeds."""
        mock_response = Mock()
        mock_response.json.return_value = {"data": "test"}
        mock_response.raise_for_status.return_value = None
        mock_get.return_value = mock_response

        result = make_request("https://example.com")
        assert result == {"data": "test"}
        mock_get.assert_called_once()

    @patch("src.scorer.requests.get")
    def test_failed_request(self, mock_get):
        """Return None on exception."""
        mock_get.side_effect = Exception("Network error")

        result = make_request("https://example.com")
        assert result is None

    @patch("src.scorer.requests.get")
    def test_request_timeout(self, mock_get):
        """Return None on timeout."""
        mock_get.side_effect = TimeoutError()

        result = make_request("https://example.com")
        assert result is None


class TestCalculateSizeScore:
    """Tests for size score calculations."""

    def test_small_model_raspberry_pi(self):
        """Small models score 1.0 on all hardware."""
        scores = calculate_size_score(0)
        assert scores["raspberry_pi"] == 1.0
        assert scores["jetson_nano"] == 1.0
        assert scores["desktop_pc"] == 1.0
        assert scores["aws_server"] == 1.0

    def test_medium_model_raspberry_pi(self):
        """Mid-size models degrade on smaller hardware."""
        scores = calculate_size_score(100)
        assert scores["raspberry_pi"] == 0.74
        assert scores["jetson_nano"] > 0.1995
        assert scores["desktop_pc"] > 0.01995
        assert scores["aws_server"] > 0.001995

    def test_large_model_all_hardware(self):
        """Large models reduce scores on constrained devices."""
        scores = calculate_size_score(10000)
        assert scores["raspberry_pi"] == 0.0
        assert scores["jetson_nano"] == 0.0
        assert scores["desktop_pc"] < 0.5
        assert scores["aws_server"] > 0.0

    def test_very_large_model(self):
        """Extremely large models yield zero scores."""
        scores = calculate_size_score(10000000)
        assert scores["raspberry_pi"] == 0.0
        assert scores["jetson_nano"] == 0.0
        assert scores["desktop_pc"] == 0.0
        assert scores["aws_server"] == 0.0

    def test_boundary_values(self):
        """Boundary cases hold expected values."""
        scores_200 = calculate_size_score(20)
        assert scores_200["raspberry_pi"] == 0.95

        scores_500 = calculate_size_score(500)
        assert scores_500["jetson_nano"] == 0.67


class TestEstimateModelSize:
    """Tests for estimate_model_size function."""

    def test_estimate_unknown_model(self):
        """Test estimation for unknown model."""
        size = estimate_model_size("unknown", "test_url", "model")
        assert size == 500

    def test_estimate_empty_model(self):
        """Test estimation for empty model name."""
        size = estimate_model_size("", "test_url", "model")
        assert size == 500

    def __test_estimate_known_model(self):
        """Test estimation for known model."""
        size = estimate_model_size(
            "google/bert",
            "https://huggingface.co/google-bert/bert-base-uncased",
            "model",
        )
        assert size == 500


class TestScoreDataset:
    """Tests for score_dataset function."""

    def test_score_dataset_invalid_url(self):
        """Test scoring invalid dataset URL."""
        result = score_dataset("https://invalid.com")
        assert result.category == UrlCategory.DATASET
        assert result.score == 0.0
        assert "error" in result.details

    @patch("src.scorer.make_request")
    def test_score_dataset_no_data(self, mock_request):
        """Test scoring dataset with no API data."""
        mock_request.return_value = None

        result = score_dataset("https://huggingface.co/datasets/squad")
        assert result.category == UrlCategory.DATASET
        assert result.details["name"] == "squad"

    @patch("src.scorer.make_request")
    def test_score_dataset_with_data(self, mock_request):
        """Test scoring dataset with API data."""
        mock_request.return_value = {
            "downloads": 50000,
            "likes": 100,
            "description": "Test dataset",
        }

        result = score_dataset("https://huggingface.co/datasets/squad")
        assert result.score > 0
        assert result.details["downloads"] == 50000
        assert result.details["likes"] == 100

    # @patch("src.scorer.make_request")
    # def test_score_dataset_high_downloads(self, mock_request):
    #     """Test scoring dataset with high downloads"""
    #     mock_request.return_value = {
    #         "downloads": 100000,
    #         "likes": 60,
    #         "description": "Test"
    #     }

    #     result = score_dataset("https://huggingface.co/datasets/test")
    #     assert result.score >= 7.0

    # @patch("src.scorer.make_request")
    # def test_score_dataset_low_metrics(self, mock_request):
    #     """Test scoring dataset with low metrics"""
    #     mock_request.return_value = {
    #         "downloads": 50,
    #         "likes": 1,
    #         "description": ""
    #     }

    #     result = score_dataset("https://huggingface.co/datasets/test")
    #     assert result.score >= 2.0
    #     assert result.score <= 4.0


class TestScoreModel:
    """Tests for score_model function."""

    def test_score_model_invalid_url(self):
        """Test scoring invalid model URL."""
        result = score_model("https://invalid.com")
        assert result.category == UrlCategory.MODEL
        assert result.score == 0.0
        assert "error" in result.details

    # @patch("src.scorer.make_request")
    # def test_score_model_no_data(self, mock_request):
    #     """Test scoring model with no API data"""
    #     mock_request.return_value = None

    #     result = score_model("https://huggingface.co/google/bert")
    #     assert result.category == UrlCategory.MODEL
    #     assert result.score == 2.0
    #     assert result.details["name"] == "google-bert/bert-case-uncased"

    # @patch("src.scorer.make_request")
    # def test_score_model_with_data(self, mock_request):
    #     """Test scoring model with API data"""
    #     mock_request.return_value = {
    #         "downloads": 200000,
    #         "likes": 150,
    #         "cardData": {"key": "value"},
    #         "pipeline_tag": "text-classification"
    #     }

    #     result = score_model("https://huggingface.co/google/bert")
    #     assert result.score > 5.0
    #     assert result.details["downloads"] == 200000
    #     assert result.details["likes"] == 150
    #     assert result.details["has_model_card"] == True
    #     assert result.details["pipeline_tag"] == "text-classification"

    # @patch("src.scorer.make_request")
    # def test_score_model_high_metrics(self, mock_request):
    #     """Test scoring model with high metrics"""
    #     mock_request.return_value = {
    #         "downloads": 500000,
    #         "likes": 200,
    #         "cardData": {},
    #         "pipeline_tag": "text-generation"
    #     }

    #     result = score_model("test_url")
    #     assert result.score >= 8.0


class TestScoreCode:
    """Tests for score_code function."""

    def test_score_code_invalid_url(self):
        """Test scoring invalid code URL."""
        result = score_code("https://invalid.com")
        assert result.category == UrlCategory.CODE
        assert result.score == 0.0
        assert "error" in result.details

    @patch("src.scorer.make_request")
    def test_score_code_no_data(self, mock_request):
        """Test scoring code with no API data."""
        mock_request.return_value = None

        result = score_code("https://github.com/user/repo")
        assert result.category == UrlCategory.CODE
        assert result.score == 2.0
        assert result.details["name"] == "user/repo"

    @patch("src.scorer.make_request")
    def test_score_code_with_data(self, mock_request):
        """Test scoring code with API data."""
        mock_request.return_value = {
            "stargazers_count": 5000,
            "forks_count": 500,
            "description": "Test repo",
            "license": {"name": "MIT"},
            "language": "Python",
        }

        result = score_code("https://github.com/user/repo")
        assert result.score > 5.0
        assert result.details["stars"] == 5000
        assert result.details["forks"] == 500
        assert result.details["has_description"] is True
        assert result.details["has_license"] is True
        assert result.details["language"] == "Python"

    @patch("src.scorer.make_request")
    def test_score_code_high_stars(self, mock_request):
        """Test scoring code with high star count."""
        mock_request.return_value = {
            "stargazers_count": 10000,
            "forks_count": 1000,
            "description": "Popular repo",
            "license": {"name": "Apache-2.0"},
            "language": "JavaScript",
        }

        result = score_code("https://github.com/popular/repo")
        assert result.score >= 8.0

    @patch("src.scorer.make_request")
    def test_score_code_low_metrics(self, mock_request):
        """Test scoring code with low metrics."""
        mock_request.return_value = {
            "stargazers_count": 5,
            "forks_count": 1,
            "description": "",
            "license": None,
            "language": None,
        }

        result = score_code("https://github.com/small/repo")
        assert result.score >= 2.0
        assert result.score <= 4.0


class TestScoreUrl:
    """Tests for score_url function."""

    @patch("src.scorer.score_dataset")
    def test_score_url_dataset(self, mock_score):
        """Test scoring dataset URL."""
        mock_result = ScoreResult(
            url="test",
            category=UrlCategory.DATASET,
            score=5.0,
            max_score=10.0,
            details={},
        )
        mock_score.return_value = mock_result

        result = score_url("https://huggingface.co/datasets/test", UrlCategory.DATASET)
        assert result.category == UrlCategory.DATASET
        mock_score.assert_called_once()

    @patch("src.scorer.score_model")
    def test_score_url_model(self, mock_score):
        """Test scoring model URL."""
        mock_result = ScoreResult(
            url="test",
            category=UrlCategory.MODEL,
            score=5.0,
            max_score=10.0,
            details={},
        )
        mock_score.return_value = mock_result

        result = score_url("https://huggingface.co/test", UrlCategory.MODEL)
        assert result.category == UrlCategory.MODEL
        mock_score.assert_called_once()

    @patch("src.scorer.score_code")
    def test_score_url_code(self, mock_score):
        """Test scoring code URL."""
        mock_result = ScoreResult(
            url="test", category=UrlCategory.CODE, score=5.0, max_score=10.0, details={}
        )
        mock_score.return_value = mock_result

        result = score_url("https://github.com/test/repo", UrlCategory.CODE)
        assert result.category == UrlCategory.CODE
        mock_score.assert_called_once()

    # def test_score_url_invalid(self):
    #     """Test scoring invalid URL"""
    #     result = score_url("https://invalid.com", UrlCategory.INVALID)
    #     assert result.category == UrlCategory.INVALID
    #     assert result.score == 0.0
    #     assert "error" in result.details


class TestBusFactor:
    """Tests for bus factor helpers."""

    def test_model_bus_factor_no_contributors(self):
        """Zero contributors yield zero score."""
        score = calculate_model_bus_factor(0, "individual/project")
        assert score == 0.0

    def test_model_bus_factor_single_contributor(self):
        """Single contributor gives partial score."""
        score = calculate_model_bus_factor(1, "individual/project")
        assert score == 0.3

    def test_model_bus_factor_major_org(self):
        """Major org boosts score with few contributors."""
        contributor_count = 1
        score = calculate_model_bus_factor(contributor_count, "microsoft/awesome-model")
        assert score == 0.95

    def test_dataset_bus_factor(self):
        """Datasets with contributors get full score."""
        contributor_count = 3
        score = calculate_dataset_bus_factor(contributor_count, "individual/dataset")
        assert score == 1.0

    def test_dataset_bus_factor_major_org(self):
        """Major org dataset gets boosted score."""
        score = calculate_dataset_bus_factor(0, "google/dataset")
        assert score == 0.95

    def test_code_bus_factor_major_org(self):
        """Major org code repo gets boosted score."""
        score = calculate_code_bus_factor(3, "openai/repo")
        assert score == 0.95

    def test_organization_detection(self):
        """Test the organization detection function."""
        assert is_major_organization("google/model") is True
        assert is_major_organization("microsoft/repo") is True
        assert is_major_organization("individual/project") is False
        assert is_major_organization("") is False

    def test_model_bus_factor_multiple_contributors(self):
        """Multiple contributors reach full score."""
        score = calculate_model_bus_factor(5, "individual/project")
        assert score == 1.0


class TestPerformanceClaims:
    """Tests for calculate_performance_claims_with_timing function."""

    def test_performance_claims_no_data(self):
        """Test performance claims with no data."""
        data = {}
        score, latency = calculate_performance_claims_with_timing(data, "")
        assert score == 0.0
        assert latency >= 0

    def test_performance_claims_with_model_card(self):
        """Test performance claims with model card."""
        data = {"cardData": {"some": "data"}, "downloads": 1000, "likes": 50}
        score, latency = calculate_performance_claims_with_timing(data, "test-model")
        assert score >= 0.1  # Should get some score for having model card
        assert latency >= 0

    def test_performance_claims_with_benchmark_keywords(self):
        """Test performance claims with benchmark keywords in card."""
        data = {
            "cardData": {
                "content": (
                    "This model achieves 95% accuracy on GLUE benchmark and shows "
                    "state-of-the-art performance"
                )
            },
            "downloads": 10000,
            "likes": 100,
        }
        score, latency = calculate_performance_claims_with_timing(
            data, "benchmark-model"
        )
        assert score > 0.3  # Should get significant score for benchmark keywords
        assert latency >= 0

    def test_performance_claims_with_benchmark_datasets(self):
        """Test performance claims with specific benchmark datasets."""
        data = {
            "cardData": {
                "content": (
                    "Results on SQuAD, SuperGLUE, and MMLU datasets show strong "
                    "performance"
                )
            },
            "downloads": 50000,
            "likes": 200,
        }
        score, latency = calculate_performance_claims_with_timing(data, "dataset-model")
        assert score > 0.2  # Should get score for benchmark datasets
        assert latency >= 0

    def test_performance_claims_with_numerical_results(self):
        """Test performance claims with numerical results."""
        data = {
            "cardData": {"content": "Accuracy: 92.5%, F1 Score: 0.89, BLEU: 45.2"},
            "downloads": 20000,
            "likes": 150,
        }
        score, latency = calculate_performance_claims_with_timing(
            data, "numerical-model"
        )
        assert score > 0.15  # Should get score for numerical results
        assert latency >= 0

    def test_performance_claims_with_performance_tags(self):
        """Test performance claims with performance-related tags."""
        data = {
            "cardData": {"content": "Model description"},
            "tags": ["benchmark", "evaluation", "sota"],
            "downloads": 30000,
            "likes": 300,
        }
        score, latency = calculate_performance_claims_with_timing(data, "tagged-model")
        assert score > 0.1  # Should get score for performance tags
        assert latency >= 0

    def test_performance_claims_with_paper_evidence(self):
        """Test performance claims with paper citations."""
        data = {
            "cardData": {
                "content": (
                    "See our paper on arXiv:1234.5678 for detailed evaluation results"
                )
            },
            "downloads": 15000,
            "likes": 100,
        }
        score, latency = calculate_performance_claims_with_timing(data, "paper-model")
        assert score > 0.1  # Should get score for paper evidence
        assert latency >= 0

    def test_performance_claims_high_popularity(self):
        """Test performance claims with high popularity."""
        data = {
            "cardData": {"content": "Basic model"},
            "downloads": 2000000,  # High downloads
            "likes": 5000,  # High likes
        }
        score, latency = calculate_performance_claims_with_timing(data, "popular-model")
        assert score > 0.15  # Should get score for high popularity
        assert latency >= 0

    def test_performance_claims_comprehensive_evidence(self):
        """Test performance claims with comprehensive evidence."""
        data = {
            "cardData": {
                "content": (
                    "This model achieves 95% accuracy on GLUE benchmark, 89% F1 on "
                    "SQuAD, and shows state-of-the-art performance. See our paper on "
                    "arXiv:1234.5678 for details."
                )
            },
            "tags": ["benchmark", "sota", "evaluation"],
            "downloads": 1000000,
            "likes": 2000,
        }
        score, latency = calculate_performance_claims_with_timing(
            data, "comprehensive-model"
        )
        assert score > 0.5  # Should get high score for comprehensive evidence
        assert latency >= 0

    def test_performance_claims_score_bounds(self):
        """Test that performance claims score is between 0 and 1."""
        data = {"cardData": {"content": "Test"}, "downloads": 1000, "likes": 10}
        score, latency = calculate_performance_claims_with_timing(data, "test-model")
        assert 0.0 <= score <= 1.0
        assert latency >= 0

    def test_performance_claims_error_handling(self):
        """Test performance claims with invalid data."""
        data = None
        score, latency = calculate_performance_claims_with_timing(data, "invalid-model")
        assert score == 0.0
        assert latency >= 0


class TestRampUpTime:
    """Tests for calculate_ramp_up_time_with_timing function."""

    def test_ramp_up_time_no_data(self):
        """Test ramp-up time with no data."""
        data = {}
        score, latency = calculate_ramp_up_time_with_timing(data, "")
        assert score == 0.0
        assert latency >= 0

    def test_ramp_up_time_with_model_card(self):
        """Test ramp-up time with model card."""
        data = {"cardData": {"some": "data"}, "downloads": 1000, "likes": 50}
        score, latency = calculate_ramp_up_time_with_timing(data, "test-model")
        assert score >= 0.1
        assert latency >= 0

    def test_ramp_up_time_with_readme_file(self):
        """Test ramp-up time with README file."""
        data = {
            "cardData": {"content": "Model description"},
            "files": [{"rfilename": "README.md"}],
            "downloads": 10000,
            "likes": 100,
        }
        score, latency = calculate_ramp_up_time_with_timing(data, "readme-model")
        assert score >= 0.24  # Should get score for model card + README file
        assert latency >= 0

    def test_ramp_up_time_with_requirements_file(self):
        """Test ramp-up time with requirements file."""
        data = {
            "cardData": {"content": "Model description"},
            "files": [{"rfilename": "requirements.txt"}],
            "downloads": 20000,
            "likes": 150,
        }
        score, latency = calculate_ramp_up_time_with_timing(data, "requirements-model")
        assert score >= 0.3  # Should get score for model card + requirements
        assert latency >= 0

    def test_ramp_up_time_with_example_script(self):
        """Test ramp-up time with example script."""
        data = {
            "cardData": {"content": "Model description"},
            "files": [{"rfilename": "example.py"}],
            "downloads": 30000,
            "likes": 200,
        }
        score, latency = calculate_ramp_up_time_with_timing(data, "example-model")
        assert score >= 0.36  # Should get score for model card + example script
        assert latency >= 0

    def test_ramp_up_time_with_config_files(self):
        """Test ramp-up time with config files."""
        data = {
            "cardData": {"content": "Model description"},
            "files": [{"rfilename": "config.json"}],
            "downloads": 40000,
            "likes": 300,
        }
        score, latency = calculate_ramp_up_time_with_timing(data, "config-model")
        assert score >= 0.3  # Should get score for model card + config
        assert latency >= 0

    def test_ramp_up_time_with_documentation_sections(self):
        """Test ramp-up time with good documentation sections."""
        data = {
            "cardData": {
                "content": (
                    "This model is for text classification. Usage: import the model. "
                    "Installation: pip install transformers. Example: see below. "
                    "Quickstart guide available."
                )
            },
            "downloads": 50000,
            "likes": 400,
        }
        score, latency = calculate_ramp_up_time_with_timing(data, "doc-model")
        assert score >= 0.3  # Should get score for model card + documentation sections
        assert latency >= 0

    def test_ramp_up_time_with_code_examples(self):
        """Test ramp-up time with code examples."""
        data = {
            "cardData": {
                "content": (
                    "```python\n"
                    "import transformers\n"
                    "from transformers import AutoModel\n"
                    'model = AutoModel.from_pretrained("model-name")\n```'
                )
            },
            "downloads": 60000,
            "likes": 500,
        }
        score, latency = calculate_ramp_up_time_with_timing(data, "code-model")
        assert score >= 0.27  # Should get score for model card + code examples
        assert latency >= 0

    def test_ramp_up_time_with_setup_instructions(self):
        """Test ramp-up time with setup instructions."""
        data = {
            "cardData": {
                "content": (
                    "Installation: pip install transformers torch. Setup: Download the "
                    "model. Install dependencies."
                )
            },
            "downloads": 70000,
            "likes": 600,
        }
        score, latency = calculate_ramp_up_time_with_timing(data, "setup-model")
        assert score >= 0.3  # Should get score for model card + setup instructions
        assert latency >= 0

    def test_ramp_up_time_comprehensive_evidence(self):
        """Test ramp-up time with comprehensive evidence."""
        data = {
            "cardData": {
                "content": (
                    "This model is for text classification. Usage: import transformers."
                    " Installation: pip install transformers. Example: "
                    '```python\nmodel = AutoModel.from_pretrained("model")\n```'
                )
            },
            "files": [
                {"rfilename": "README.md"},
                {"rfilename": "requirements.txt"},
                {"rfilename": "example.py"},
                {"rfilename": "config.json"},
            ],
            "downloads": 1000000,
            "likes": 2000,
        }
        score, latency = calculate_ramp_up_time_with_timing(data, "comprehensive-model")
        assert score >= 0.8  # Should get high score for comprehensive evidence
        assert latency >= 0

    def test_ramp_up_time_high_popularity(self):
        """Test ramp-up time with high popularity."""
        data = {
            "cardData": {"content": "Basic model"},
            "downloads": 2000000,  # High downloads
            "likes": 5000,  # High likes
        }
        score, latency = calculate_ramp_up_time_with_timing(data, "popular-model")
        assert score >= 0.2  # Should get score for high popularity
        assert latency >= 0

    def test_ramp_up_time_score_bounds(self):
        """Test that ramp-up time score is between 0 and 1."""
        data = {"cardData": {"content": "Test"}, "downloads": 1000, "likes": 10}
        score, latency = calculate_ramp_up_time_with_timing(data, "test-model")
        assert 0.0 <= score <= 1.0
        assert latency >= 0

    def test_ramp_up_time_error_handling(self):
        """Test ramp-up time with invalid data."""
        data = None
        score, latency = calculate_ramp_up_time_with_timing(data, "invalid-model")
        assert score == 0.0
        assert latency >= 0
